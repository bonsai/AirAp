# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from transformers import (
    AutoModelForCausalLM, # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼ˆGPT-2ã¨ã‹ï¼‰ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã®æ©Ÿèƒ½
    AutoTokenizer,       # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆæ–‡ç« ã‚’ãƒ¢ãƒ‡ãƒ«ãŒç†è§£ã§ãã‚‹æ•°å­—ã«å¤‰ãˆã‚‹æ©Ÿèƒ½ï¼‰
    TrainingArguments,   # ç‰¹è¨“ï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰ã®è¨­å®š
    Trainer,             # ç‰¹è¨“ã‚’å®Ÿè¡Œã™ã‚‹æ©Ÿèƒ½
)
from datasets import load_dataset, Dataset # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ‰±ã†æ©Ÿèƒ½

# 1. è¶…é‡è¦ï¼ç‰¹è¨“ã§ä½¿ã†ãƒ¢ãƒ‡ãƒ«åã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å
MODEL_NAME = "rinna/japanese-gpt2-medium"

# 2. ç‰¹è¨“ã®ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹
# Dockerfileã§ã“ã®å ´æ‰€ã«ç½®ã„ãŸã€Œãƒ©ãƒƒãƒ—ã®æ­Œè©ãƒ‡ãƒ¼ã‚¿ã€ã‚’ä½¿ã†ã‚ˆï¼
DATA_PATH = "rap_lyrics.txt"

# 3. ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ï¼
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)

# GPT-2ç³»ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€Œæ–‡ç« ã®åˆ‡ã‚Œç›®ã€ã‚’æ•™ãˆã‚‹ãŸã‚ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ãŒãªã„å ´åˆãŒã‚ã‚‹ã‹ã‚‰è¿½åŠ ã™ã‚‹ã¹ã—ï¼
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    model.resize_token_embeddings(len(tokenizer))


# 4. ãƒ©ãƒƒãƒ—ã®æ­Œè©ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨æ„ã™ã‚‹ã¹ã—ï¼
# load_datasetã§ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå½¢å¼ã«ã™ã‚‹
raw_datasets = load_dataset('text', data_files={'train': DATA_PATH})

# æ­Œè©ã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆãƒ¢ãƒ‡ãƒ«ãŒç†è§£ã§ãã‚‹å½¢å¼ã«å¤‰æ›ï¼‰ã™ã‚‹é–¢æ•°
def tokenize_function(examples):
    # max_lengthã¯ãƒ©ãƒƒãƒ—ã®1è¡Œã‚„ãƒ–ãƒ­ãƒƒã‚¯ã®é•·ã•ã«åˆã‚ã›ã¦èª¿æ•´ã™ã‚‹ã¨ã„ã„ã‚ˆï¼
    return tokenizer(examples["text"], truncation=True, max_length=128)

tokenized_datasets = raw_datasets.map(
    tokenize_function,
    batched=True,
    num_proc=4, # PCã®ã‚³ã‚¢æ•°ã«åˆã‚ã›ã¦ä¸¦åˆ—å‡¦ç†ã™ã‚‹ã¨çˆ†é€Ÿï¼
    remove_columns=["text"],
)

# ãƒ¢ãƒ‡ãƒ«ã®ç‰¹è¨“ã«å¿…è¦ãªå½¢ã«ãƒ‡ãƒ¼ã‚¿ã‚’æœ€çµ‚åŠ å·¥ï¼
lm_datasets = tokenized_datasets.map(
    lambda x: {"labels": x["input_ids"].copy()}, # å…¥åŠ›ã‚’ãã®ã¾ã¾ç­”ãˆï¼ˆãƒ©ãƒ™ãƒ«ï¼‰ã«ã™ã‚‹
    batched=True,
)


# 5. ç‰¹è¨“ã®è¨­å®šã‚’æ±ºã‚ã‚‹ã¹ã—ï¼ï¼ˆã“ã®è¨­å®šã§ãƒ¢ãƒ‡ãƒ«ã®å‡ºæ¥ãŒæ±ºã¾ã‚‹ï¼ï¼‰
training_args = TrainingArguments(
    output_dir="./rap_model_results", # ç‰¹è¨“å¾Œã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹å ´æ‰€
    num_train_epochs=3,               # ç‰¹è¨“ã®å›æ•°ï¼ˆå›æ•°ãŒå¤šã™ãã‚‹ã¨éå­¦ç¿’ã«ãªã£ã¡ã‚ƒã†ã‹ã‚‰æ³¨æ„ï¼ï¼‰
    per_device_train_batch_size=4,    # ä¸€åº¦ã«å­¦ç¿’ã•ã›ã‚‹ãƒ‡ãƒ¼ã‚¿ã®é‡ï¼ˆGPUã®ãƒ¡ãƒ¢ãƒªã«åˆã‚ã›ã¦èª¿æ•´ï¼ï¼‰
    learning_rate=5e-5,               # å­¦ç¿’ã®é€Ÿã•
    save_total_limit=2,               # ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã™ã‚‹æ•°
    logging_dir='./logs',
    logging_steps=100,
    fp16=True,                        # GPUã‚’ä½¿ã†ãªã‚‰ã€å‡¦ç†ã‚’é€Ÿãã™ã‚‹ãŸã‚ã®è¨­å®šï¼ˆå¿…é ˆï¼ï¼‰
    report_to="none",                 # ä»Šå›ã¯ãƒ­ã‚®ãƒ³ã‚°æ©Ÿèƒ½ã‚’ä½¿ã‚ãªã„
)

# 6. ç‰¹è¨“ãƒã‚·ãƒ¼ãƒ³ï¼ˆTrainerï¼‰ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_datasets["train"],
    tokenizer=tokenizer,
)

# 7. ç‰¹è¨“ã‚¹ã‚¿ãƒ¼ãƒˆï¼ãƒã‚¸ã§ã‚¢ãƒ„ã„ï¼
print("ğŸ”¥ ç‰¹è¨“ï¼ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰é–‹å§‹ï¼ã‚¢ã‚²ã€œï¼ ğŸ”¥")
trainer.train()

# 8. å®Œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã¹ã—ï¼
trainer.save_model("./best_rap_model")
print("âœ… ç‰¹è¨“å®Œäº†ï¼æœ€å¼·ã®ãƒ©ãƒƒãƒ—ãƒ¢ãƒ‡ãƒ«ãŒ './best_rap_model' ã«çˆ†èª•ã—ãŸã‚ˆï¼")