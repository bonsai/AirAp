"""
æ­Œè©ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
MLæ©Ÿèƒ½çµ±åˆç‰ˆ: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã€å“è³ªè©•ä¾¡ã€æ”¹å–„ææ¡ˆã‚’å«ã‚€
"""
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
)
from datasets import load_dataset, Dataset
from pathlib import Path
from typing import Optional, List, Dict
from loguru import logger
import json

# MLåˆ†ææ©Ÿèƒ½ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from .lyrics_ml import get_analyzer, analyze_lyrics
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    logger.warning("ML analysis features not available")

# ============================================================
# è¨­å®š
# ============================================================

# 1. ãƒ¢ãƒ‡ãƒ«å
MODEL_NAME = "rinna/japanese-gpt2-medium"

# 2. ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹
DATA_PATH = "rap_lyrics.txt"
OUTPUT_DIR = "./rap_model_results"
BEST_MODEL_DIR = "./best_rap_model"

# 3. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š
NUM_EPOCHS = 3
BATCH_SIZE = 4
LEARNING_RATE = 5e-5
MAX_LENGTH = 128

# ============================================================
# ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã¨å“è³ªè©•ä¾¡
# ============================================================

def preprocess_and_filter_data(data_path: str, min_quality: float = 0.5) -> List[str]:
    """
    ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†ã—ã€å“è³ªã®ä½ã„æ­Œè©ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    
    Args:
        data_path: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        min_quality: æœ€å°å“è³ªã‚¹ã‚³ã‚¢
    
    Returns:
        ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸæ­Œè©ã®ãƒªã‚¹ãƒˆ
    """
    logger.info(f"Loading and preprocessing data from {data_path}")
    
    with open(data_path, 'r', encoding='utf-8') as f:
        raw_lyrics = f.read().strip().split('\n\n')  # ç©ºè¡Œã§åŒºåˆ‡ã‚‰ã‚ŒãŸæ­Œè©
    
    if not ML_AVAILABLE:
        logger.warning("ML analysis not available, skipping quality filtering")
        return raw_lyrics
    
    analyzer = get_analyzer()
    filtered_lyrics = []
    
    for i, lyrics in enumerate(raw_lyrics):
        if not lyrics.strip():
            continue
        
        try:
            analysis = analyzer.analyze_lyrics(lyrics)
            
            if analysis.quality.overall_score >= min_quality:
                filtered_lyrics.append(lyrics)
            else:
                logger.debug(f"Filtered out lyrics {i+1}: quality={analysis.quality.overall_score:.2f}")
        except Exception as e:
            logger.warning(f"Error analyzing lyrics {i+1}: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å«ã‚ã‚‹
            filtered_lyrics.append(lyrics)
    
    logger.info(f"Filtered {len(raw_lyrics)} -> {len(filtered_lyrics)} lyrics")
    return filtered_lyrics


def save_quality_report(lyrics_list: List[str], output_path: str):
    """å“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜"""
    if not ML_AVAILABLE:
        return
    
    analyzer = get_analyzer()
    report = {
        "total_lyrics": len(lyrics_list),
        "analyses": []
    }
    
    for lyrics in lyrics_list[:100]:  # æœ€åˆã®100ä»¶ã®ã¿
        try:
            analysis = analyzer.analyze_lyrics(lyrics)
            report["analyses"].append({
                "quality_score": analysis.quality.overall_score,
                "rhyme_score": analysis.quality.rhyme_score,
                "rhythm_score": analysis.quality.rhythm_score,
                "sentiment": analysis.sentiment.dominant_emotion,
                "theme": analysis.theme,
                "word_count": analysis.word_count
            })
        except Exception:
            pass
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    logger.info(f"Quality report saved to {output_path}")


# ============================================================
# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
# ============================================================

logger.info(f"Loading model: {MODEL_NAME}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)

# ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®è¿½åŠ 
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    model.resize_token_embeddings(len(tokenizer))


# 4. ãƒ©ãƒƒãƒ—ã®æ­Œè©ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨æ„ã™ã‚‹ã¹ã—ï¼
# load_datasetã§ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå½¢å¼ã«ã™ã‚‹
raw_datasets = load_dataset('text', data_files={'train': DATA_PATH})

# æ­Œè©ã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆãƒ¢ãƒ‡ãƒ«ãŒç†è§£ã§ãã‚‹å½¢å¼ã«å¤‰æ›ï¼‰ã™ã‚‹é–¢æ•°
def tokenize_function(examples):
    # max_lengthã¯ãƒ©ãƒƒãƒ—ã®1è¡Œã‚„ãƒ–ãƒ­ãƒƒã‚¯ã®é•·ã•ã«åˆã‚ã›ã¦èª¿æ•´ã™ã‚‹ã¨ã„ã„ã‚ˆï¼
    return tokenizer(examples["text"], truncation=True, max_length=128)

tokenized_datasets = raw_datasets.map(
    tokenize_function,
    batched=True,
    num_proc=4, # PCã®ã‚³ã‚¢æ•°ã«åˆã‚ã›ã¦ä¸¦åˆ—å‡¦ç†ã™ã‚‹ã¨çˆ†é€Ÿï¼
    remove_columns=["text"],
)

# ãƒ¢ãƒ‡ãƒ«ã®ç‰¹è¨“ã«å¿…è¦ãªå½¢ã«ãƒ‡ãƒ¼ã‚¿ã‚’æœ€çµ‚åŠ å·¥ï¼
lm_datasets = tokenized_datasets.map(
    lambda x: {"labels": x["input_ids"].copy()}, # å…¥åŠ›ã‚’ãã®ã¾ã¾ç­”ãˆï¼ˆãƒ©ãƒ™ãƒ«ï¼‰ã«ã™ã‚‹
    batched=True,
)


# ============================================================
# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š
# ============================================================

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    learning_rate=LEARNING_RATE,
    save_total_limit=2,
    logging_dir='./logs',
    logging_steps=100,
    fp16=True,  # GPUä½¿ç”¨æ™‚
    report_to="none",
    save_strategy="epoch",
    evaluation_strategy="no",  # è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯"no"
)

# ============================================================
# ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
# ============================================================

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_datasets["train"],
    tokenizer=tokenizer,
)

# ============================================================
# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
# ============================================================

if __name__ == "__main__":
    logger.info("ğŸ”¥ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹ï¼ ğŸ”¥")
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
    train_result = trainer.train()
    
    # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    logger.info(f"Saving model to {BEST_MODEL_DIR}")
    trainer.save_model(BEST_MODEL_DIR)
    tokenizer.save_pretrained(BEST_MODEL_DIR)
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çµæœã®ä¿å­˜
    metrics = {
        "train_loss": train_result.training_loss,
        "epochs": NUM_EPOCHS,
        "batch_size": BATCH_SIZE,
        "learning_rate": LEARNING_RATE
    }
    
    with open(f"{OUTPUT_DIR}/training_metrics.json", 'w') as f:
        json.dump(metrics, f, indent=2)
    
    logger.success(f"âœ… ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼ãƒ¢ãƒ‡ãƒ«ã¯ '{BEST_MODEL_DIR}' ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
    logger.info(f"æœ€çµ‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æå¤±: {train_result.training_loss:.4f}")